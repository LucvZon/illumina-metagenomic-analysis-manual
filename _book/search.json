[
  {
    "objectID": "quality_control.html",
    "href": "quality_control.html",
    "title": "2  Quality control",
    "section": "",
    "text": "2.1 Decompressing FASTQ\nFor simplicity’s sake, we will go through the steps for an individual sample. It is recommended to follow a basic file structure like the following below:\nThe first step is to decompress the raw FASTQ files. FASTQ files are often compressed using gzip to save disk space. We’ll use the zcat command to decompress them.\nModify and run:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Illumina metagenomic data analysis",
    "section": "",
    "text": "Introduction\nWelcome to the Illumina metagenomic data analysis manual. This manual contains a step by step guide for performing quality control, filtering host sequences, assembling reads into contigs, annotating the contigs, and then extracing viral contigs and their corresponding reads. In the final chapter of the manual we will show how to automate all of these steps into a single pipeline for speed and convenience.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "preparation.html",
    "href": "preparation.html",
    "title": "1  Preparation",
    "section": "",
    "text": "1.1 Activating the correct conda software environment\nAnaconda is a software management tool that can be used for creating specific environments where bioinformatics software can be installed in Linux, as it manages all the dependencies of different softwares for you. To run the steps outlined in this manual, be sure to first activate the proper conda environment.\nActivate a conda environment can be done by copying and executing the following code:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparation</span>"
    ]
  },
  {
    "objectID": "preparation.html#activating-the-correct-conda-software-environment",
    "href": "preparation.html#activating-the-correct-conda-software-environment",
    "title": "1  Preparation",
    "section": "",
    "text": "conda activate {environment}\n\n{environment} is the name of your chosen conda environment.\n\n\n\n\n\n\n\nNote\n\n\n\nWe are now ready to start executing the code to perform quality control of our raw Nanopore sequencing data in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparation</span>"
    ]
  },
  {
    "objectID": "assembly.html",
    "href": "assembly.html",
    "title": "3  De novo assembly",
    "section": "",
    "text": "3.1 metaSPades\nWe will perform de novo assembly of the host-filtered reads to create contigs (longer, assembled sequences) with SPades.\nNext we will combine all contigs into a single fasta file so we can perform taxonomic annotation across all samples in one go. Before performing this step, we will add sample names to the beginning of each contig name. This will allow us to split annotation files back into individual annotation files for each sample.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>De novo assembly</span>"
    ]
  },
  {
    "objectID": "annotation.html",
    "href": "annotation.html",
    "title": "4  Taxonomic classification",
    "section": "",
    "text": "4.1 Diamond\nNow we will annotate the aggregated contigs by assigning taxonomic classifications to them based on sequence similarity to known proteins in a database using diamond blastx.\nMultiple different databases and taxomaps can be found in: \\\\cifs.research.erasmusmc.nl\\viro0002\\workgroups_projects\\Bioinformatics\\DB",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Taxonomic classification</span>"
    ]
  },
  {
    "objectID": "parse_annotation.html",
    "href": "parse_annotation.html",
    "title": "5  Final steps (think of name)",
    "section": "",
    "text": "5.1 Extract viral annotations\nWe will conclude the pipeline with various steps which will create valuable data files for further downstream analysis.\nA basic grep command can be used to extract contigs that have been annotated as viral from the annotation file in step 4.3.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Final steps (think of name)</span>"
    ]
  },
  {
    "objectID": "illumina_hpc.html",
    "href": "illumina_hpc.html",
    "title": "6  Automating data analysis",
    "section": "",
    "text": "6.1 Preparing to run the workflow.\nWe can make use of a tool called Snakemake to automate the previous steps into a single pipeline. To set this up, you must create a tabular config file, which contains …\nThe tabular config file has the following structure:\n[insert tabular example file]\nNext, we need to create a place for your results to be stored. First check your current directory with the pwd command:\nChange your current directory using cd if needed:\nThen create the new directory using the mkdir command:\nThis creates a new directory at the current location. Move the sample_config.tsv file to the results directory using the mv (move) command:\nAfter preparing the results directory we have to make sure that we have activated the appropriate conda environment and check if the command line has (your_environment_name) in front.\nNext, run the ls command to list the files in the current directory and check if the InsertCorrectName.smk file is present. This is the “recipe” for the workflow, describing all the steps we have done by hand. (you can open the .smk file with a text editor and have a look).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Automating data analysis</span>"
    ]
  },
  {
    "objectID": "illumina_hpc.html#connecting-to-a-server",
    "href": "illumina_hpc.html#connecting-to-a-server",
    "title": "6  Illumina metagenomic data analysis on HPC",
    "section": "",
    "text": "snakemake \\\n--snakefile \\\nSnakefile.smk \\\n--directory {ourdir} \\\n--configfile sample_config={config} \\\n--cores {threads}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Automating data analysis</span>"
    ]
  },
  {
    "objectID": "quality_control.html#decompressing-fastq",
    "href": "quality_control.html#decompressing-fastq",
    "title": "2  Quality control",
    "section": "",
    "text": "zcat {input.folder}/{sample}_R1_001.fastq.gz &gt; {output.folder}/{sample}_R1.fastq\nzcat {input.folder}/{sample}_R2_001.fastq.gz &gt; {output.folder}/{sample}_R2.fastq\n\n{input.folder} is where your raw .fastq.gz data is stored.\n{sample} is the name of your sample.\n{output.folder} is where your decompressed .fastq files will be stored.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "quality_control.html#running-fastp-quality-control-software",
    "href": "quality_control.html#running-fastp-quality-control-software",
    "title": "2  Quality control",
    "section": "2.3 Running fastp quality control software",
    "text": "2.3 Running fastp quality control software\nThe fastp software is a very fast multipurpose quality control software to perform quality and sequence adapter trimming for Illumina short-read and Nanopore long-read data.\nBecause we are processing Illumina short-read data,\n        fastp -i {input.R1} -I {input.R2} \\\n        -o {output.R1} -O {output.R2} \\\n        --unpaired1 {output.S} --unpaired2 {output.S} --failed_out {output.failed} \\\n        --length_required 50 \\\n        --low_complexity_filter \\\n        --cut_right \\\n        --cut_right_window_size 5 \\\n        --cut_right_mean_quality 25 \\\n        --thread {threads} \\\n        -j result/{sample}/dedup_qc/qc_report.json -h result/{sample}/dedup_qc/qc_report.html &gt; {log} 2&gt;&1\n\n{input.R1} and {input.R2} are the reads belonging to the deduplicated sample step 2.2.\n{output.R1} and {output.R2} are the the quality controlled .fastq filenames.\n{output.S} –unpaired1 and –unpaired2 tells fastp to write unpaired reads to a .fastq file. In our case, we write unpaired reads (whether they originated from the R1 or R2 file) to the same file, output.S.\n{output.failed} .fastq file that stores reads (either merged or unmerged) which failed the quality filters\n{sample} is the name of your sample.\n{log} is the directory for log files.\n\n\n\n\n\n\n\nNote\n\n\n\n{threads} is a recurring setting for the number of CPUs to use for the processing. On a laptop this will be less (e.g. 8), on an HPC you may be able to use 64 or more CPUs for processing. However, how much performance increase you get depends on the software.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "quality_control.html#deduplicate-reads",
    "href": "quality_control.html#deduplicate-reads",
    "title": "2  Quality control",
    "section": "2.2 Deduplicate reads",
    "text": "2.2 Deduplicate reads\nThis step removes duplicate reads from the uncompressed FASTQ files. Duplicate reads can arise during PCR amplification or sequencing and can skew downstream analyses. We’ll use the cd-hit-dup program to identify and remove these duplicates.\nAfter cd-hit-dup command is finished, we’ll remove the .clstr file that cd-hit-dup creates. This file contains information about the clusters of duplicate reads, but it’s not needed for downstream analysis, so we can safely remove it to save disk space.\ncd-hit-dup -u 50 -i {input.R1} -i2 {input.R2} -o {output.R1} -o2 {output.R2} &gt; {logs}/{sample}_cd-hit.log 2&gt;&1\n\nrm {output.dir}/*.clstr\n\n{input.R1} and {input.R2} are the decompressed R1 and R2 reads from step 2.1.\n{output.R1} and {output.R2} are your deduplicated .fastq reads. Think of where you want to store your results, something like results/dedup/ will be sufficient, so output.R1 turns into result/dedup/SampleName_R1.fastq, etc.\n{output.dir} This should be pointed to wherever your deduplicated reads are stored.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "assembly.html#metaspades",
    "href": "assembly.html#metaspades",
    "title": "3  De novo assembly",
    "section": "",
    "text": "COUNT=$(cat {input.S} | wc -l)\nif [[ $COUNT -gt 0 ]]\nthen\n    spades.py -t {threads} \\\n    --meta \\\n    -o {params.spades_folder} \\\n    -1 {input.R1} \\\n    -2 {input.R2} \\\n    -s {input.S} &gt; {log} 2&gt;&1\nelse\n    spades.py -t {threads} \\\n    --meta \\\n    -o {params.spades_folder} \\\n    -1 {input.R1} \\\n    -2 {input.R2} &gt; {log} 2&gt;&1\nfi    \n\n{input.R1}, {input.R2} and {input.S} are host-filtered FASTQ files (R1, R2, and S) from step 2.3.\n{params.spades_folder} defines the directory where the assembly results will be stored.\n{log} specifies the log directory.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>De novo assembly</span>"
    ]
  },
  {
    "objectID": "quality_control.html#host-filtering",
    "href": "quality_control.html#host-filtering",
    "title": "2  Quality control",
    "section": "2.4 Host filtering",
    "text": "2.4 Host filtering\nThis step removes reads that map to a host genome (e.g., human). This is important if you’re studying metagenomes from a host-associated environment (e.g., gut microbiome, stool).\nbwa mem -aY -t {threads} {params.reference} {input.R1} {input.R2} | \\\nsamtools fastq -f 4 -s /dev/null -1 {output.R1} -2 {output.R2} -\nbwa mem -aY -t {threads} {params.reference}  {input.S} | \\\nsamtools fastq -f 4 - &gt; {output.S}\n\n{input.R1} and {input.R2} are QC-filtered FASTQ files from step 2.3.\n{output.R1} and {output.R2} are FASTQ files (R1, R2) containing reads that did not map to the host genome.\n{input.S} are singleton reads from the previous step (output.S).\n{params.reference} is a reference genome sequence.\n\n\n\n\n\n\n\nNote\n\n\n\nWe now have our quality controlled sequence reads which we can use to create an assembly in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "annotation.html#diamond",
    "href": "annotation.html#diamond",
    "title": "4  Taxonomic classification",
    "section": "",
    "text": "diamond blastx \\\n        --frameshift 15 \\\n        -q {input} \\\n        -d {db} \\\n        -o {output} \\\n        -f 6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore staxids \\\n        --taxonmap {tx} \\\n        --threads {threads} \\\n        -b 10 -c 1 &gt; {log} 2&gt;&1\n\n{input} is the aggregated contig file created in step 3.3.\n{db} is the protein database to be searched against.\n{output} is a .tsv file containing the annotation results.\n{tx} is a mapping file to convert protein accessions to taxonomic IDs.\n{log} is the log directory.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Taxonomic classification</span>"
    ]
  },
  {
    "objectID": "annotation.html#split-annotation-files",
    "href": "annotation.html#split-annotation-files",
    "title": "4  Taxonomic classification",
    "section": "4.2 Split annotation files",
    "text": "4.2 Split annotation files\nWe will split the combined annotation file back into individual annotation files for each sample.\n        mkdir -p tmp_split\n\n        sed 's/_NODE/|NODE/' {input} | awk -F'|' '{{\n            identifier = $1;  # Construct the identifier using the first two fields\n\n            output_file = \"tmp_split/\" identifier;  # Construct the output filename\n\n            if (!seen[identifier]++) {{\n                close(output_file);  # Close the previous file (if any)\n                output = output_file;  # Update the current output file\n            }}\n\n            print $2 &gt; output;  # Append the line to the appropriate output file\n        }}'\n\n        for file in tmp_split/*; do\n            mkdir -p result/$(basename ${{file}})/annotation/;\n            mv $file result/$(basename ${{file}})/annotation/diamond_output.tsv;\n        done\n\n        rmdir tmp_split\n\n{input} is the combined annotation file from step 4.1.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Taxonomic classification</span>"
    ]
  },
  {
    "objectID": "annotation.html#parsing-diamond-output",
    "href": "annotation.html#parsing-diamond-output",
    "title": "4  Taxonomic classification",
    "section": "4.3 Parsing diamond output",
    "text": "4.3 Parsing diamond output\nNow we will process the DIAMOND output files with a custom Python script called post_process_diamond.py. This script will further enrich taxonomic information for each contig based on the DIAMOND alignment results. If a contig has multiple matches in the database, it will select the best hit based on a combined score of bitscore and length. Lastly, it separates the contigs into two lists: those that were successfully annotated and unannotated.\n        python /mnt/viro0002/workgroups_projects/Bioinformatics/scripts/post_process_diamond.py \\\n        -i {input.annotation} \\\n        -c {input.contigs} \\\n        -o {output.annotated} \\\n        -u {output.unannotated} \\\n        -log {log}\n\n{input.annotation} is the annotation file step 4.2.\n{input.contigs} are the contigs from the SPAdes step 3.1.\n{output.annotated} is a set of annotated contigs.\n{output.unannotated} is a set of unannotated contig IDs.\n{log} is the log directory.\n\n[Add note here…]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Taxonomic classification</span>"
    ]
  },
  {
    "objectID": "parse_annotation.html#extract-viral-annotations",
    "href": "parse_annotation.html#extract-viral-annotations",
    "title": "5  Final steps (think of name)",
    "section": "",
    "text": "grep \"Viruses$\" {input.annotated} &gt; {output.viral} || touch {output.viral}\n\n{input.annotated} is the annotation file from step 4.3.\n{output.viral} contains all of your viral contigs.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Final steps (think of name)</span>"
    ]
  },
  {
    "objectID": "parse_annotation.html#mapping-reads-to-contigs",
    "href": "parse_annotation.html#mapping-reads-to-contigs",
    "title": "5  Final steps (think of name)",
    "section": "5.2 Mapping reads to contigs",
    "text": "5.2 Mapping reads to contigs\nWe can map the quality-filtered and host-filtered reads back to the assembled contigs to quantify the abundance of different contigs in each sample. We will create a mapping file for paired reads (R1 and R2) and singletons (S).\nbwa index {input.contigs}\nbwa mem -Y -t {threads} {input.contigs} {input.R1} {input.R2} | samtools sort - &gt; result/{sample}/mapping/tmp_paired.bam\nbwa mem -Y -t {threads} {input.contigs} {input.S} | samtools sort - &gt; result/{sample}/mapping/tmp_singlets.bam\nsamtools merge {output} result/{sample}/mapping/tmp_paired.bam result/{wildcards.sample}/mapping/tmp_singlets.bam\nrm result/{sample}/mapping/tmp_paired.bam result/{sample}/mapping/tmp_singlets.bam\n\n{sample} is the name of your sample.\n{input.contigs} contains the assembled contigs from step 3.1.\n{input.R1}, {input.R2} and {input.S} are the quality controlled and host-filtered reads from step 2.4.\n{output} is a .bam mapping file",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Final steps (think of name)</span>"
    ]
  },
  {
    "objectID": "parse_annotation.html#extract-mapped-reads",
    "href": "parse_annotation.html#extract-mapped-reads",
    "title": "5  Final steps (think of name)",
    "section": "5.3 Extract mapped reads",
    "text": "5.3 Extract mapped reads\nWe will extract the reads for each annotation file that we’ve created.\n#Create temporary BED file to extract the annotated mappings from the BAM of all mappings\ncut -f1 {input.annotated} | awk -F'_' '{{print $0 \"\\t\" 0 \"\\t\" $4}}' &gt; result/{sample}/mapping/tmp.bed\nsamtools view -bL result/{sample}/mapping/tmp.bed {input.mapped} &gt; {output.annotated}\n      \n#Do the same for the unannotated contigs\ncut -f1 {input.unannotated} | awk -F'_' '{{print $0 \"\\t\" 0 \"\\t\" $4}}' &gt; result/{sample}/mapping/tmp.bed\nsamtools view -bL result/{sample}/mapping/tmp.bed {input.mapped} &gt; {output.unannotated}\n\n#Do the same for the viral contigs\ncut -f1 {input.viral} | awk -F'_' '{{print $0 \"\\t\" 0 \"\\t\" $4}}' &gt; result/{sample}/mapping/tmp.bed\nsamtools view -bL result/{sample}/mapping/tmp.bed {input.mapped} &gt; {output.viral}\n\nrm result/{sample}/mapping/tmp.bed\n\n{input.annotated}, {input.unannotated} and {input.viral} are the .tsv annotation files step 4.3 and 5.1.\n{input.mapped} the combined .bam file from step 5.2.\n{output.annotated}, {output.unannotated} and {output.viral} are .bam files for each annotation input.\n{sample} is the name of your sample.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Final steps (think of name)</span>"
    ]
  },
  {
    "objectID": "parse_annotation.html#count-mapped-reads",
    "href": "parse_annotation.html#count-mapped-reads",
    "title": "5  Final steps (think of name)",
    "section": "5.4 Count mapped reads",
    "text": "5.4 Count mapped reads\nNext we count the number of reads that mapped to each contig in the annotated, unannotated, and viral BAM files.\nsamtools view -bF2052 {input.annotated} | seqkit bam -Qc - | awk '$2 != 0 {{print}}' &gt; {output.annotated}\nsamtools view -bF2052 {input.unannotated} | seqkit bam -Qc - | awk '$2 != 0 {{print}}' &gt; {output.unannotated}\nsamtools view -bF2052 {input.viral} | seqkit bam -Qc - | awk '$2 != 0 {{print}}' &gt; {output.viral}\n\n{input.annotated}, {input.unannotated} and {input.viral} are the .bam files from step 5.3.\n{output.annotated}, {output.unannotated} and {output.viral} are .tsv files containing the read counts for each .bam file",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Final steps (think of name)</span>"
    ]
  },
  {
    "objectID": "parse_annotation.html#extract-contigs",
    "href": "parse_annotation.html#extract-contigs",
    "title": "5  Final steps (think of name)",
    "section": "5.5 Extract contigs",
    "text": "5.5 Extract contigs\nLastly, we will extract contigs based on the output from the annotation annotation files.\nseqkit grep -f &lt;(cut -f1 {input.annotated}) {input.contigs} &gt; {output.annotated}\nseqkit grep -f &lt;(cut -f1 {input.unannotated}) {input.contigs} &gt; {output.unannotated}\nseqkit grep -f &lt;(cut -f1 {input.viral}) {input.contigs} &gt; {output.viral}\n\n{input.annotated}, {input.unannotated} and {input.viral} are .tsv annotation files from steps 4.3 and 5.1.\n{input.contigs} is the .fasta file containing all contigs for a sample from step 3.1.\n{output.annotated}, {output.unannotated} and {output.viral} are .fasta files containing the contigs.\n\n\n\n\n\n\n\nNote\n\n\n\nYou can now move to the final chapter to automate all of the steps we’ve previously discussed.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Final steps (think of name)</span>"
    ]
  },
  {
    "objectID": "illumina_hpc.html#preparing-to-run-the-workflow.",
    "href": "illumina_hpc.html#preparing-to-run-the-workflow.",
    "title": "6  Automating data analysis",
    "section": "",
    "text": "pwd\n\ncd /{folder1}/{folder2}\n\nmkdir results\n\nmv sample_config.tsv results\n\n\n\n\n\n\nTip\n\n\n\nIf you get a “file not found” error after changing your directory (cd) you may need to write the complete path of your sample_config.tsv file e.g mv /home/username/Documents/sample_config.tsv results",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Automating data analysis</span>"
    ]
  },
  {
    "objectID": "illumina_hpc.html#running-the-workflow",
    "href": "illumina_hpc.html#running-the-workflow",
    "title": "6  Automating data analysis",
    "section": "6.2 Running the workflow",
    "text": "6.2 Running the workflow\nAfter setting everything up, we can redo the analysis for all samples in a single step. First we will test out a dry run to see if any errors appear. A dry run will not execute any of the commands but will instead display what would be done. This will help identify any errors in the Snakemake or config file.\nsnakemake \\\n--snakefile \\\nSnakefile.smk \\\n--directory {ourdir} \\\n--configfile sample_config={config} \\\n--cores {threads}\n--dryrun\n\n{ourdir} is the directly we created.\n{config} is the tabular config file.\n\nIf no errors appear, then remove the --dryrun argument and run it again to fully execute the workflow.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Automating data analysis</span>"
    ]
  },
  {
    "objectID": "assembly.html#renaming-contigs",
    "href": "assembly.html#renaming-contigs",
    "title": "3  De novo assembly",
    "section": "3.2 Renaming contigs:",
    "text": "3.2 Renaming contigs:\nseqkit replace -p \"^\" -r \"{sample}_\" {input} &gt; {output}\n\n{sample} is the sample name that will be placed in front of the contig name\n{input} is your contig file from step 3.1\n{output} is fasta file with the renamed contig",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>De novo assembly</span>"
    ]
  },
  {
    "objectID": "assembly.html#aggregating-contigs",
    "href": "assembly.html#aggregating-contigs",
    "title": "3  De novo assembly",
    "section": "3.3 Aggregating contigs:",
    "text": "3.3 Aggregating contigs:\ncat {input} &gt; {output}\n\n{input} are your renamed contig files from step 3.2\n{output} a single .fasta file\n\n\n\n\n\n\n\nNote\n\n\n\nWe have created contigs that are ready for taxonomic annotation in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>De novo assembly</span>"
    ]
  }
]